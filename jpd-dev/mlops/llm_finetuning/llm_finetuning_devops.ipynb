{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Qwen 1.5 Model and Logging to a Model Registry\n",
    "\n",
    "This notebook demonstrates the process of fine-tuning a small-scale Qwen model (`Qwen/Qwen1.5-0.5B-Chat`) on a public instruction-based dataset. We will use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to make the process memory-efficient.\n",
    "\n",
    "**Key Steps:**\n",
    "1.  **Setup**: Install required libraries and import necessary modules.\n",
    "2.  **Configuration**: Define all parameters for the model, dataset, and training.\n",
    "3.  **Data Preparation**: Load and prepare the dataset for instruction fine-tuning.\n",
    "4.  **Model Loading and Fine-Tuning**: Load the pre-trained model and tokenizer, and then fine-tune it using `trl`'s `SFTTrainer`.\n",
    "5.  **Evaluation**: Compare the performance of the base model with the fine-tuned model.\n",
    "6.  **Model Logging**: Log the fine-tuned model and its metrics to a model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we'll install the necessary Python libraries and import all the required modules for the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We'll define all our configurations in one place. This makes the notebook cleaner and easier to modify for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# 导入frogml库（假设这是JFrog集成的自定义库），可以用于：\n",
    "# 模型/数据集的上传下载\n",
    "# 与JFrog Artifactory的集成\n",
    "# 模型版本管理和跟踪\n",
    "import frogml # Assuming frogml is the library for your JFrog integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbconvert import export\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_name = \"Szaid3680/Devops\"\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned\",  # 训练输出目录，用于保存检查点、日志和最终模型\n",
    "    per_device_train_batch_size=1,  # 每个设备的训练批次大小，设为1可能是因为内存限制或使用较大模型\n",
    "    gradient_accumulation_steps=8,  # 梯度累积步数，通过8次前向传播累积梯度再更新权重，等效批次大小=1×8=8\n",
    "    learning_rate=2e-4,  # 学习率，控制模型权重更新的步长\n",
    "    logging_steps=10,   # 每10步记录一次训练日志\n",
    "    max_steps=100,  # 最大训练步数，只训练100步（可能是演示或快速测试）\n",
    "    fp16=False,  # 禁用FP16混合精度训练，注释说明这是为了兼容CPU或MPS（Apple Silicon）设备\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We will load the `Szaid3680/Devops` dataset, split it into training and evaluation sets, and define a formatting function for instruction-based fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the training dataset:\n",
      "{'Response': '\\n\\n\\n\\n\\n\\n\\n\\n            1\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou should not use $request_uri to construct the rewritten URI, as it also contains ?limit=all - so your rewritten URI would look like: /clothing.html?limit=all?product_list_limit=all\\nThe rewrite statement resets numeric captures, so you will need to use a named capture in the if statement.\\nThe URI without query string is available as $uri or as a capture from the rewrite statement\\'s regular expression.\\nEither of these forms should work for you:\\nif ($query_string ~* \"^limit=(?<limit>.*)$\") {\\n    rewrite ^(.*)$ $1?product_list_limit=$limit? redirect;\\n}\\n\\nNote the trailing ? to prevent the original query string from being appended. See this document for more.\\nOr:\\nif ($query_string ~* \"^limit=(.*)$\") {\\n    return 302 $uri?product_list_limit=$1;\\n}\\n\\nThe value of the ?limit=all0 argument is also available as ?limit=all1, so you could also use this:\\n?limit=all2\\nSee this document for details.\\n\\n\\n\\n\\n\\n\\n\\n\\nShare\\n\\n\\n\\n                        Follow\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            answered Jul 27, 2018 at 9:45\\n\\n\\n\\n\\n\\n\\nRichard SmithRichard Smith\\n\\n47.6k66 gold badges8888 silver badges8585 bronze badges\\n\\n\\n\\n\\n\\n\\n\\n0\\n\\n\\n\\n\\n\\n\\nAdd a comment\\n\\xa0|\\xa0\\n\\n\\n\\n\\n', 'Instruction': '\\nWe were running on apache previously and used the below code to redirect/replace one of our query parameters.\\nRewriteCond %{QUERY_STRING} ^limit=(.*)$ [NC]\\nRewriteRule ^ %{REQUEST_URI}?product_list_limit=%1 [L,R=301]\\n\\nThis would successfully redirect the below URL\\nwww.example.com/clothing.html?limit=all\\n\\nTO\\nwww.example.com/clothing.html?product_list_limit=all\\n\\nI have used a htaccess redirect convertor that has produced the below. This doesn\\'t work\\n    location ~ / {\\n    if ($query_string ~* \"^limit=(.*)$\"){\\n        rewrite ^(.*)$ /$request_uri?product_list_limit=%1 redirect;\\n    }\\n}\\n\\nAny help appreciated \\nThanks\\n', 'Prompt': 'Nginx Replace Query string redirect'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# For a quick demo, we'll use a small subset of the data\n",
    "train_dataset = train_dataset.select(range(2))\n",
    "eval_dataset = eval_dataset.select(range(2))\n",
    "\n",
    "# 定义一个数据格式化函数，将原始数据转换为模型训练所需的格式\n",
    "# 转换示例如下：\n",
    "# # 原始数据\n",
    "# {\n",
    "#     'Instruction': '解释Docker', \n",
    "#     'Prompt': '用简单的话说明', \n",
    "#     'Response': 'Docker是容器化技术...'\n",
    "# }\n",
    "# # 格式化后\n",
    "# \"<s>[INST] 解释Docker\\n用简单的话说明 [/INST] Docker是容器化技术... </s>\"\n",
    "def format_instruction(example):\n",
    "    \"\"\"Formats the dataset examples into a structured prompt.\"\"\"\n",
    "    instruction = example.get('Instruction', '')\n",
    "    inp = example.get('Prompt', '')\n",
    "    response = example.get('Response', '')\n",
    "    \n",
    "    full_prompt = f\"<s>[INST] {instruction}\\n{inp} [/INST] {response} </s>\"\n",
    "    return full_prompt\n",
    "\n",
    "# Let's look at a sample from the training set\n",
    "print(\"Sample from the training dataset:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading and Fine-Tuning\n",
    "\n",
    "Now, we'll load the base model and tokenizer. Then, we will apply the LoRA configuration and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HF_HUB_ETAG_TIMEOUT: 86400\n",
      "✅ HF_HUB_DOWNLOAD_TIMEOUT: 86400\n",
      "✅ HF_ENDPOINT: https://<JPD_URL>/artifactory/api/huggingfaceml/slash-project-slash-project-huggingface-remote\n",
      "✅ HF_TOKEN: <access_token>\n",
      "模型下载到: /Users/jingyil/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/snapshots/7a2b85d322d12d10e07300b3d2742e4da7377821\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# 通过代码设置环境变量（不推荐，因为会暴露敏感信息）\n",
    "os.environ['HF_HUB_ETAG_TIMEOUT'] = '86400'\n",
    "os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '86400'\n",
    "os.environ['HF_ENDPOINT'] = 'https://<JPD_URL>/artifactory/api/huggingfaceml/slash-project-slash-project-huggingface-remote'\n",
    "os.environ['HF_TOKEN'] = '<access_token>'\n",
    "\n",
    "# 检查环境变量是否设置成功\n",
    "required_vars = ['HF_HUB_ETAG_TIMEOUT', 'HF_HUB_DOWNLOAD_TIMEOUT', 'HF_ENDPOINT', 'HF_TOKEN']\n",
    "for var in required_vars:\n",
    "    value = os.environ.get(var)\n",
    "    if value:\n",
    "        print(f\"✅ {var}: {value}\")\n",
    "    else:\n",
    "        print(f\"❌ {var}: 未设置\")\n",
    "\n",
    "# Model and tokenizer configuration\n",
    "model_id = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "new_model_adapter = \"qwen-0.5b-devops-adapter\"\n",
    "\n",
    "# 1. 下载模型到本地缓存\n",
    "local_dir = snapshot_download(\n",
    "    repo_id = model_id, revision=\"main\", etag_timeout=86400, \n",
    "    endpoint=os.environ['HF_ENDPOINT'],\n",
    "    token=os.environ['HF_TOKEN'],\n",
    ")\n",
    "\n",
    "print(f\"模型下载到: {local_dir}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingyil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingyil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:40, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.008400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_dir,\n",
    "    device_map=\"cpu\" # Use CPU for local demo\n",
    ")\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Create the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=format_instruction,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"--- Starting Fine-Tuning ---\")\n",
    "trainer.train()\n",
    "print(\"--- Fine-Tuning Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Let's evaluate the fine-tuned model and compare its response to the base model's response for a sample DevOps-related prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingyil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "{'eval_loss': 5.826877593994141, 'eval_runtime': 0.6542, 'eval_samples_per_second': 3.057, 'eval_steps_per_second': 1.528, 'eval_entropy': 1.8605223894119263, 'eval_num_tokens': 87800.0, 'eval_mean_token_accuracy': 0.2784184515476227, 'epoch': 100.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model adapter\n",
    "trainer.model.save_pretrained(new_model_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- FINE-TUNED MODEL RESPONSE -------------------\n",
      "system\n",
      "You are a helpful DevOps assistant.\n",
      "user\n",
      "How do I expose a deployment in Kubernetes using a service?\n",
      "assistant\n",
      "To expose a deployment in Kubernetes using a service, you can follow these steps:\n",
      "\n",
      "1. Create a YAML file for the service:\n",
      "```\n",
      "apiVersion: apps/v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: my-deployment\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: my-deployment\n",
      "  ports:\n",
      "    - containerPort: 80\n",
      "```\n",
      "\n",
      "2. Create a JSON object to describe the service:\n",
      "```\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"name\": \"my-deployment\"\n",
      "  },\n",
      "  \"spec\": {\n",
      "    \"port\": [\n",
      "      { port: 80, protocol: \"tcp\", name: \"default-port\" }\n",
      "    ],\n",
      "    \"type\": \"services\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "3. Update the Deployment metadata in Kubernetes to reference the Service:\n",
      "```\n",
      "kubectl apply -f my-service.yaml\n",
      "```\n",
      "\n",
      "4. Start or stop the Service as needed to ensure that it is running.\n",
      "5. To check if the Service is available through the API, you can use the following command:\n",
      "```\n",
      "$ kubectl get services -n my-deployment\n",
      "Name         Type                    Address                        Port                           Class               Comment\n",
      "---              ------                    -------                         -------------                 ---------                             </NAME>\n",
      "my-service       int6\n",
      "\n",
      "------------------- BASE MODEL RESPONSE -------------------\n",
      "system\n",
      "You are a helpful DevOps assistant.\n",
      "user\n",
      "How do I expose a deployment in Kubernetes using a service?\n",
      "assistant\n",
      "In Kubernetes, you can expose a deployment by creating a deployment group that defines the resources required for the deployment and then creating a Deployment object to expose those resources.\n",
      "Here's an example of how you can create a deployment in Kubernetes:\n",
      "```yaml\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: my-deployment\n",
      "spec:\n",
      "  replicas: 3\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: my-app\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: my-app\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: my-container\n",
      "        image: my-image\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "```\n",
      "\n",
      "In this example, we're creating a deployment named \"my-deployment\" with three replicas of a containerized application named \"my-container\". The deployment is defined as a deployment group, which is a group of resources that can be shared across multiple applications within a cluster. In this case, we've defined three replicas of a containerized application, each running on port 80.\n",
      "To deploy this deployment, you can use the `kubectl apply` command to create a file called `deployment.yaml`, where you can define the resources required for the deployment and then specify the replicas of the deployment\n"
     ]
    }
   ],
   "source": [
    "# Merge the LoRA adapter with the base model for easy inference\n",
    "base_model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"cpu\")\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, new_model_adapter)\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "# Define a prompt for evaluation\n",
    "prompt = \"How do I expose a deployment in Kubernetes using a service?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful DevOps assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate response from the fine-tuned model\n",
    "print(\"------------------- FINE-TUNED MODEL RESPONSE -------------------\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cpu\")\n",
    "generated_ids = finetuned_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "response_finetuned = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(response_finetuned)\n",
    "\n",
    "# Generate response from the original base model for comparison\n",
    "print(\"\\n------------------- BASE MODEL RESPONSE -------------------\")\n",
    "original_model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"cpu\")\n",
    "generated_ids_base = original_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "response_base = tokenizer.decode(generated_ids_base[0], skip_special_tokens=True)\n",
    "print(response_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Logging\n",
    "\n",
    "Finally, we log our fine-tuned model, its tokenizer, and the evaluation metrics to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:HuggingfaceModelVersionManager:Logging model slash-finetuned_qwen to slash-project-ml-test-local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get requested logger name HuggingfaceModelVersionManager. Using default logger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JmlCustomerClient:Customer exists in JML.\n",
      "INFO:JmlCustomerClient:Getting project key for repository slash-project-ml-test-local\n",
      "INFO:frogml.sdk.model_version.utils.files_tools:Code directory, predict file and dependencies are provided. Setup template files for model_name slash-finetuned_qwen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3626fe574c48473a98b11e5c4f7fa624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/add…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5d86137c5f48f7bb68964c4eab0175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/con…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b90a6cb26648ab870cb1dae673c47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/spe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f76f2b3214340378b6b2ba4ad30e0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/tok…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a81852e1d8c4877b9d30beb3fbdf2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/tok…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53907d604ff44a4eae1e817fa4a9794b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/mer…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3245aba92a6493e9a807ce379cdd510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/gen…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058e42c6d47a435795428f759282f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/cha…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f9a87de1a249f7af98a8b2b3c7ca49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/mod…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02045d2ccf1a47a49cdf1b6ccd6e5ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/private/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/slash-finetuned_qwen.pretrained_model/voc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06df990a6484458a781bf325cbfc34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/Users/jingyil/work/jfrog/jpd-project/jpd-dev/mlops/llm_finetuning/main/conda.yaml:   0%|          | 0.00/284 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4f4c59202f4f80a6a376fc0d984415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/var/folders/6n/71s0t39j0tn2wz0lp1jn9pv80000gp/T/tmpkkqndwgx/code.zip:   0%|          | 0.00/2.60k [00:00<?, ?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-11 11:40:02,469 - INFO - frogml.storage.logging._log_config.frog_ml.__upload_model:540 - Model: \"slash-finetuned_qwen\", version: \"1.3.0\" has been uploaded successfully\n",
      "--- Model Logged Successfully ---\n"
     ]
    }
   ],
   "source": [
    "# REPLACE WITH YOUR OWN FILESYSTEM BASE PATH WHERE THE PROJECTS RESIDE\n",
    "base_projects_directory = \"/Users/jingyil/work/jfrog/jpd-project/jpd-dev/mlops\"\n",
    "\n",
    "try:\n",
    "    import frogml\n",
    "\n",
    "    frogml.huggingface.log_model(   \n",
    "        model= finetuned_model,\n",
    "        tokenizer= tokenizer,\n",
    "        repository=\"slash-project-ml-test-local\",    # The JFrog repository to upload the model to.\n",
    "        model_name=\"slash-finetuned_qwen\",     # The uploaded model name\n",
    "        version=\"1.3.0\",     # Optional. The uploaded model version\n",
    "        parameters={\"finetuning-dataset\": dataset_name},\n",
    "        code_dir=f\"{base_projects_directory}/llm_finetuning/code_dir\",\n",
    "        dependencies=[f\"{base_projects_directory}/llm_finetuning/main/conda.yaml\"],\n",
    "        metrics = metrics,\n",
    "        predict_file=f\"{base_projects_directory}/llm_finetuning/code_dir/predict.py\"\n",
    "    )\n",
    "    print(\"--- Model Logged Successfully ---\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model logging: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
